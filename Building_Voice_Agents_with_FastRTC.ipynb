{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gyaneshhere/VoiceAIInterface/blob/main/Building_Voice_Agents_with_FastRTC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f30a0d9-7ef1-4135-a3ea-085c1f83bc0b",
      "metadata": {
        "id": "7f30a0d9-7ef1-4135-a3ea-085c1f83bc0b"
      },
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/aimug-org/austin_langchain/blob/main/labs/LangChain_112/Building_Voice_Agents_with_FastRTC.ipynb)  \n",
        "\n",
        "# Building Voice Agents with FastRTC\n",
        "\n",
        "This notebook will walk you through setting up and testing a simple FastRTC server.  \n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "1. `uv` from [astral.sh](https://docs.astral.sh/uv/) to manage python environments. (Already present in Google Colab)\n",
        "2. OpenAI API Key from https://platform.openai.com/settings/organization/api-keys available as environment variable `OPENAI_API_KEY` or as Google Colab Secrets\n",
        "3. (Optional for FastRTC Client WebPage test) Ngrok AuthToken from https://dashboard.ngrok.com/get-started/your-authtoken available as environment variable `NGROK_AUTH_TOKEN` or as Google Colab Secrets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69438025-932f-47a5-9782-b79744604bdf",
      "metadata": {
        "id": "69438025-932f-47a5-9782-b79744604bdf"
      },
      "source": [
        "## Initialize UV project and virtual environment\n",
        "\n",
        "Initialize a new `uv` project with `uv init`.  \n",
        "Pin Python to version 3.13  \n",
        "Create a new virtual environment for the project.\n",
        "\n",
        "Note: `export VIRTUAL_ENV=` is needed only within Jupyter notebooks and can be excluded otherwise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7025c387-8e83-431d-8458-18d284b55e01",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7025c387-8e83-431d-8458-18d284b55e01",
        "outputId": "7ec27571-96d9-48e6-8089-ce0f8310595a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated `.python-version` from `3.11` -> `3.13`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Initialized project `content`\n",
            "warning: No interpreter found for Python 3.13 in managed installations or search path\n",
            "warning: The `--system` flag has no effect, a system Python interpreter is always used in `uv venv`\n",
            "Downloading cpython-3.13.2-linux-x86_64-gnu (20.4MiB)\n",
            " Downloaded cpython-3.13.2-linux-x86_64-gnu\n",
            "Using CPython 3.13.2\n",
            "Creating virtual environment at: .venv\n",
            "Activate with: source .venv/bin/activate\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "export VIRTUAL_ENV=\n",
        "uv init\n",
        "uv python pin 3.13\n",
        "uv venv"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a01b3cf5-5282-4c4d-abbc-a39eb6d75639",
      "metadata": {
        "id": "a01b3cf5-5282-4c4d-abbc-a39eb6d75639"
      },
      "source": [
        "## Add FastRTC to the project dependencies\n",
        "\n",
        "Note: `export VIRTUAL_ENV=.venv` is needed only within Jupyter notebooks and can be excluded otherwise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d58e4c0f-2f02-4bb3-ac1d-41224cfe9579",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d58e4c0f-2f02-4bb3-ac1d-41224cfe9579",
        "outputId": "ad927812-4244-4905-c794-0da29de335cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Resolved 118 packages in 1.41s\n",
            "Downloading hf-xet (51.1MiB)\n",
            "Downloading numba (3.7MiB)\n",
            "Downloading tokenizers (2.9MiB)\n",
            "Downloading scipy (35.5MiB)\n",
            "Downloading aiortc (1.8MiB)\n",
            "Downloading pygments (1.2MiB)\n",
            "Downloading cryptography (4.0MiB)\n",
            "Downloading sympy (6.0MiB)\n",
            "Downloading llvmlite (40.4MiB)\n",
            "Downloading pydantic-core (1.9MiB)\n",
            "Downloading babel (9.7MiB)\n",
            "Downloading pandas (12.1MiB)\n",
            "Downloading pillow (4.4MiB)\n",
            "Downloading onnxruntime (15.6MiB)\n",
            "Downloading gradio (51.6MiB)\n",
            "Downloading soundfile (1.3MiB)\n",
            "Downloading fastrtc (1.9MiB)\n",
            "Downloading numpy (15.4MiB)\n",
            "Downloading scikit-learn (12.6MiB)\n",
            "Downloading espeakng-loader (9.6MiB)\n",
            "Downloading av (33.5MiB)\n",
            "Downloading pylibsrtp (2.1MiB)\n",
            "Downloading ruff (11.0MiB)\n",
            " Downloaded soundfile\n",
            " Downloaded aiortc\n",
            " Downloaded pydantic-core\n",
            " Downloaded fastrtc\n",
            " Downloaded pylibsrtp\n",
            " Downloaded pygments\n",
            " Downloaded tokenizers\n",
            " Downloaded pillow\n",
            " Downloaded cryptography\n",
            " Downloaded numba\n",
            " Downloaded ruff\n",
            " Downloaded babel\n",
            " Downloaded sympy\n",
            " Downloaded pandas\n",
            " Downloaded espeakng-loader\n",
            " Downloaded scikit-learn\n",
            " Downloaded numpy\n",
            " Downloaded onnxruntime\n",
            " Downloaded av\n",
            " Downloaded llvmlite\n",
            " Downloaded hf-xet\n",
            " Downloaded scipy\n",
            " Downloaded gradio\n",
            "Prepared 116 packages in 15.21s\n",
            "Installed 116 packages in 325ms\n",
            " + aiofiles==24.1.0\n",
            " + aioice==0.10.1\n",
            " + aiortc==1.11.0\n",
            " + annotated-types==0.7.0\n",
            " + anyio==4.9.0\n",
            " + attrs==25.3.0\n",
            " + audioop-lts==0.2.1\n",
            " + audioread==3.0.1\n",
            " + av==14.3.0\n",
            " + babel==2.17.0\n",
            " + certifi==2025.4.26\n",
            " + cffi==1.17.1\n",
            " + charset-normalizer==3.4.2\n",
            " + click==8.2.0\n",
            " + colorama==0.4.6\n",
            " + coloredlogs==15.0.1\n",
            " + colorlog==6.9.0\n",
            " + cryptography==44.0.3\n",
            " + csvw==3.5.1\n",
            " + decorator==5.2.1\n",
            " + dlinfo==2.0.0\n",
            " + dnspython==2.7.0\n",
            " + espeakng-loader==0.2.4\n",
            " + fastapi==0.115.12\n",
            " + fastrtc==0.0.23\n",
            " + fastrtc-moonshine-onnx==20241016\n",
            " + ffmpy==0.5.0\n",
            " + filelock==3.18.0\n",
            " + flatbuffers==25.2.10\n",
            " + fsspec==2025.3.2\n",
            " + google-crc32c==1.7.1\n",
            " + gradio==5.29.0\n",
            " + gradio-client==1.10.0\n",
            " + groovy==0.1.2\n",
            " + h11==0.16.0\n",
            " + hf-xet==1.1.0\n",
            " + httpcore==1.0.9\n",
            " + httpx==0.28.1\n",
            " + huggingface-hub==0.31.1\n",
            " + humanfriendly==10.0\n",
            " + idna==3.10\n",
            " + ifaddr==0.2.0\n",
            " + isodate==0.7.2\n",
            " + jinja2==3.1.6\n",
            " + joblib==1.5.0\n",
            " + jsonschema==4.23.0\n",
            " + jsonschema-specifications==2025.4.1\n",
            " + kokoro-onnx==0.4.9\n",
            " + language-tags==1.2.0\n",
            " + lazy-loader==0.4\n",
            " + librosa==0.11.0\n",
            " + llvmlite==0.44.0\n",
            " + markdown-it-py==3.0.0\n",
            " + markupsafe==3.0.2\n",
            " + mdurl==0.1.2\n",
            " + mpmath==1.3.0\n",
            " + msgpack==1.1.0\n",
            " + numba==0.61.2\n",
            " + numpy==2.2.5\n",
            " + onnxruntime==1.22.0\n",
            " + orjson==3.10.18\n",
            " + packaging==25.0\n",
            " + pandas==2.2.3\n",
            " + phonemizer-fork==3.3.2\n",
            " + pillow==11.2.1\n",
            " + platformdirs==4.3.8\n",
            " + pooch==1.8.2\n",
            " + protobuf==6.30.2\n",
            " + pycparser==2.22\n",
            " + pydantic==2.11.4\n",
            " + pydantic-core==2.33.2\n",
            " + pydub==0.25.1\n",
            " + pyee==13.0.0\n",
            " + pygments==2.19.1\n",
            " + pylibsrtp==0.12.0\n",
            " + pyopenssl==25.0.0\n",
            " + pyparsing==3.2.3\n",
            " + python-dateutil==2.9.0.post0\n",
            " + python-multipart==0.0.20\n",
            " + pytz==2025.2\n",
            " + pyyaml==6.0.2\n",
            " + rdflib==7.1.4\n",
            " + referencing==0.36.2\n",
            " + regex==2024.11.6\n",
            " + requests==2.32.3\n",
            " + rfc3986==1.5.0\n",
            " + rich==14.0.0\n",
            " + rpds-py==0.24.0\n",
            " + ruff==0.11.9\n",
            " + safehttpx==0.1.6\n",
            " + scikit-learn==1.6.1\n",
            " + scipy==1.15.3\n",
            " + segments==2.3.0\n",
            " + semantic-version==2.10.0\n",
            " + shellingham==1.5.4\n",
            " + six==1.17.0\n",
            " + sniffio==1.3.1\n",
            " + soundfile==0.13.1\n",
            " + soxr==0.5.0.post1\n",
            " + standard-aifc==3.13.0\n",
            " + standard-chunk==3.13.0\n",
            " + standard-sunau==3.13.0\n",
            " + starlette==0.46.2\n",
            " + sympy==1.14.0\n",
            " + threadpoolctl==3.6.0\n",
            " + tokenizers==0.21.1\n",
            " + tomlkit==0.13.2\n",
            " + tqdm==4.67.1\n",
            " + typer==0.15.3\n",
            " + typing-extensions==4.13.2\n",
            " + typing-inspection==0.4.0\n",
            " + tzdata==2025.2\n",
            " + uritemplate==4.1.1\n",
            " + urllib3==2.4.0\n",
            " + uvicorn==0.34.2\n",
            " + websockets==15.0.1\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "export VIRTUAL_ENV=.venv\n",
        "uv add fastrtc[vad,tts,stt]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "395233ae-ca85-4ac5-acbb-cca70c9ce57f",
      "metadata": {
        "id": "395233ae-ca85-4ac5-acbb-cca70c9ce57f"
      },
      "source": [
        "## Echo Server from fastrtc.org [QuickStart](https://fastrtc.org/#__tabbed_1_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d057c1a0-715c-47ba-9f5e-f86a5f75f5f8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d057c1a0-715c-47ba-9f5e-f86a5f75f5f8",
        "outputId": "91c4fa32-17ef-4226-b9d2-f19f1a161c01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "from fastrtc import Stream, ReplyOnPause\n",
        "import numpy as np\n",
        "\n",
        "def echo(audio: tuple[int, np.ndarray]):\n",
        "    # The function will be passed the audio until the user pauses\n",
        "    # Implement any iterator that yields audio\n",
        "    # See \"LLM Voice Chat\" for a more complete example\n",
        "    yield audio\n",
        "\n",
        "stream = Stream(\n",
        "    handler=ReplyOnPause(echo),\n",
        "    modality=\"audio\",\n",
        "    mode=\"send-receive\",\n",
        "    # below rtc_configuration needed to work around potential firewall issues\n",
        "    rtc_configuration={\n",
        "        \"iceServers\": [{ \"urls\": [\"stun:stun.l.google.com:19302\"] }]\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0860819d-f42d-4568-9aa1-17c5df22487e",
      "metadata": {
        "id": "0860819d-f42d-4568-9aa1-17c5df22487e"
      },
      "source": [
        "## Make it a Gradio App"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69d97832-2974-43b2-8004-c1f2c47430e7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69d97832-2974-43b2-8004-c1f2c47430e7",
        "outputId": "6fb764aa-8a23-4c39-ff96-1ba6451f043d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appending to app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile -a app.py\n",
        "stream.ui.launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "badd1525-f0ec-4757-a8ff-e7fd7fba42a7",
      "metadata": {
        "id": "badd1525-f0ec-4757-a8ff-e7fd7fba42a7"
      },
      "source": [
        "## Running Gradio App"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ea939b3-ac06-425f-95ef-14fdede68c62",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ea939b3-ac06-425f-95ef-14fdede68c62",
        "outputId": "8645de79-612b-4a4f-8c73-4a9924fa0320"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/.venv/lib/python3.13/site-packages/pydub/utils.py:300: SyntaxWarning: invalid escape sequence '\\('\n",
            "  m = re.match('([su]([0-9]{1,2})p?) \\(([0-9]{1,2}) bit\\)$', token)\n",
            "/content/.venv/lib/python3.13/site-packages/pydub/utils.py:301: SyntaxWarning: invalid escape sequence '\\('\n",
            "  m2 = re.match('([su]([0-9]{1,2})p?)( \\(default\\))?$', token)\n",
            "/content/.venv/lib/python3.13/site-packages/pydub/utils.py:310: SyntaxWarning: invalid escape sequence '\\('\n",
            "  elif re.match('(flt)p?( \\(default\\))?$', token):\n",
            "/content/.venv/lib/python3.13/site-packages/pydub/utils.py:314: SyntaxWarning: invalid escape sequence '\\('\n",
            "  elif re.match('(dbl)p?( \\(default\\))?$', token):\n",
            "silero_vad.onnx: 100% 1.81M/1.81M [00:00<00:00, 10.1MB/s]\n",
            "\u001b[32mINFO\u001b[0m:\t  Warming up VAD model.\n",
            "\u001b[32mINFO\u001b[0m:\t  VAD model warmed up.\n",
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "* Running on public URL: https://048f4530b21e5e550a.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://048f4530b21e5e550a.gradio.live\n",
            "\u001b[0mTask was destroyed but it is pending!\n",
            "task: <Task pending name='Task-1575' coro=<WebRTCConnectionMixin.handle_offer.<locals>._() running at /content/.venv/lib/python3.13/site-packages/fastrtc/webrtc_connection_mixin.py:321> wait_for=<Future pending cb=[Task.task_wakeup()]> cb=[AsyncIOEventEmitter._emit_run.<locals>.callback() at /content/.venv/lib/python3.13/site-packages/pyee/asyncio.py:97]>\n"
          ]
        }
      ],
      "source": [
        "!VIRTUAL_ENV=.venv GRADIO_SHARE=\"True\" uv run app.py"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "977b6a57-a9ce-4c76-bb84-d1000d84ee58",
      "metadata": {
        "id": "977b6a57-a9ce-4c76-bb84-d1000d84ee58"
      },
      "source": [
        "## FastRTC using FastAPI backend and HTML frontend\n",
        "\n",
        "`nest-asyncio` and `pyngrok` are only needed for testing from within Google Colab.  \n",
        "These can be excluded otherwise."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "export VIRTUAL_ENV=.venv\n",
        "uv add nest-asyncio pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCpRlPw8OGhN",
        "outputId": "1aef70a5-22fe-42a7-e2a0-f9e5eb5ef009"
      },
      "id": "KCpRlPw8OGhN",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Resolved 120 packages in 220ms\n",
            "Prepared 2 packages in 66ms\n",
            "Installed 2 packages in 4ms\n",
            " + nest-asyncio==1.6.0\n",
            " + pyngrok==7.2.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dee9031c-5bb0-4c7d-875e-6a62c7d98888",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dee9031c-5bb0-4c7d-875e-6a62c7d98888",
        "outputId": "50504a64-1bb2-4245-86f0-bc1fd5fe4d87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "from fastrtc import Stream, ReplyOnPause\n",
        "import numpy as np\n",
        "from fastapi import FastAPI\n",
        "from fastapi.responses import HTMLResponse\n",
        "import os\n",
        "\n",
        "# Below imports are only needed to get this example to work within Google Colab\n",
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Below two lines are only needed to get this to work within Google Colab\n",
        "NGROK_AUTH_TOKEN = os.getenv(\"NGROK_AUTH_TOKEN\")\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "def echo(audio: tuple[int, np.ndarray]):\n",
        "    # The function will be passed the audio until the user pauses\n",
        "    # Implement any iterator that yields audio\n",
        "    # See \"LLM Voice Chat\" for a more complete example\n",
        "    yield audio\n",
        "\n",
        "stream = Stream(\n",
        "    handler=ReplyOnPause(echo),\n",
        "    modality=\"audio\",\n",
        "    mode=\"send-receive\",\n",
        "    rtc_configuration={\n",
        "        \"iceServers\": [{ \"urls\": [\"stun:stun.l.google.com:19302\"] }]\n",
        "    }\n",
        ")\n",
        "app = FastAPI()\n",
        "stream.mount(app)\n",
        "\n",
        "# Optional: Add routes\n",
        "@app.get(\"/\")\n",
        "async def _():\n",
        "    return HTMLResponse(content=open(\"index.html\").read())\n",
        "\n",
        "# Below lines are only needed to get this example to work within Google Colab\n",
        "ngrok_tunnel = ngrok.connect(8000)\n",
        "print('Public URL:', ngrok_tunnel.public_url)\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b83b81b6-e2ef-4a97-ac53-27befdc85b37",
      "metadata": {
        "id": "b83b81b6-e2ef-4a97-ac53-27befdc85b37"
      },
      "source": [
        "### Client Html Page"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d0dcd44-18d0-41a6-9386-488f7c9abef1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d0dcd44-18d0-41a6-9386-488f7c9abef1",
        "outputId": "fd1a483a-2990-49d4-bf20-2282087c3155"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing index.html\n"
          ]
        }
      ],
      "source": [
        "%%writefile index.html\n",
        "<html>\n",
        "\n",
        "<head>\n",
        "  <title>FastRTC Client Demo</title>\n",
        "  <script src=\"https://cdn.jsdelivr.net/gh/lalanikarim/fastrtc-client@v0.1.2/fastrtc-client.js\"></script>\n",
        "</head>\n",
        "\n",
        "<body>\n",
        "  <h1>FastRTC Echo Server</h1>\n",
        "  <button id=\"start\">Connect</button>\n",
        "  <button id=\"stop\" style=\"display: none\">Disconnect</button>\n",
        "  <h3>Logs</h3>\n",
        "  <pre class=\"logs\"></pre>\n",
        "  <audio></audio>\n",
        "  <script defer>\n",
        "    let logs = document.querySelector(\"pre\")\n",
        "    let startButton = document.querySelector(\"button#start\")\n",
        "    let stopButton = document.querySelector(\"button#stop\")\n",
        "    let client = FastRTCClient({\n",
        "      additional_outputs_url: null,\n",
        "      // below rtc_config is needed to work around potential firewall issues\n",
        "      rtc_config: {\n",
        "        iceServers: [\n",
        "          {\n",
        "            urls: [\"stun:stun.l.google.com:19302\"]\n",
        "          }\n",
        "        ]\n",
        "      }\n",
        "    })\n",
        "    client.onConnecting(() => {\n",
        "      logs.innerText += \"Connecting to server.\\n\"\n",
        "      startButton.style.display = \"none\"\n",
        "      stopButton.style.display = \"block\"\n",
        "    })\n",
        "    client.onConnected(() => {\n",
        "      logs.innerText += \"Connected to server.\\n\"\n",
        "    })\n",
        "    client.onReadyToConnect(() => {\n",
        "      logs.innerText += \"Not connected to server.\\n\"\n",
        "      startButton.style.display = \"block\"\n",
        "      stopButton.style.display = \"none\"\n",
        "    })\n",
        "    client.onErrorReceived((error) => {\n",
        "      logs.innerText += `serverError received: ${error}\\n`\n",
        "    })\n",
        "    client.onPauseDetectedReceived(() => {\n",
        "      logs.innerText += `pause detected event received. response will start now.\\n`\n",
        "    })\n",
        "    client.onResponseStarting(() => {\n",
        "      logs.innerText += `response starting event received. audio will start playing now.\\n`\n",
        "    })\n",
        "    client.setShowErrorCallback((error) => {\n",
        "      logs.innerText += `showError received: ${error}\\n`\n",
        "    })\n",
        "    startButton.addEventListener(\"click\", () => client.start())\n",
        "    stopButton.addEventListener(\"click\", () => client.stop())\n",
        "  </script>\n",
        "</body>\n",
        "\n",
        "</html>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d33e2ea3-19d5-402c-95e5-5a1c9efcf222",
      "metadata": {
        "id": "d33e2ea3-19d5-402c-95e5-5a1c9efcf222"
      },
      "source": [
        "### Testing webpage locally\n",
        "\n",
        "In order to test the web client locally, you can run the below command.  \n",
        "WebRTC will work locally over http over localhost.  \n",
        "In order to use another domain, WebRTC will only work over https."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "ngrok_auth_token = userdata.get('NGROK_AUTH_TOKEN')"
      ],
      "metadata": {
        "id": "7S0v6hKxPdIv"
      },
      "id": "7S0v6hKxPdIv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8307f37b-590d-47a0-9151-946f68024d09",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8307f37b-590d-47a0-9151-946f68024d09",
        "outputId": "af4cce2a-3d2a-41d3-d3ca-08f519a24b18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32mINFO\u001b[0m:\t  Warming up VAD model.\n",
            "\u001b[32mINFO\u001b[0m:\t  VAD model warmed up.\n",
            "Public URL: https://c3af-34-106-222-65.ngrok-free.app\n",
            "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m4516\u001b[0m]\n",
            "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
            "\u001b[32mINFO\u001b[0m:\t  Visit \u001b[36mhttps://fastrtc.org/userguide/api/\u001b[0m for WebRTC or Websocket API docs.\n",
            "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
            "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://127.0.0.1:8000\u001b[0m (Press CTRL+C to quit)\n",
            "\u001b[32mINFO\u001b[0m:     2600:1700:1f3:25f0:e0ce:4e3b:7d6f:68b4:0 - \"\u001b[1mGET / HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     2600:1700:1f3:25f0:e0ce:4e3b:7d6f:68b4:0 - \"\u001b[1mGET /favicon.ico HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     2600:1700:1f3:25f0:e0ce:4e3b:7d6f:68b4:0 - \"\u001b[1mPOST /webrtc/offer HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     Shutting down\n",
            "\u001b[32mINFO\u001b[0m:     Waiting for application shutdown.\n",
            "\u001b[32mINFO\u001b[0m:     Application shutdown complete.\n",
            "\u001b[32mINFO\u001b[0m:     Finished server process [\u001b[36m4516\u001b[0m]\n",
            "\u001b[0mTask was destroyed but it is pending!\n",
            "task: <Task pending name='Task-1348' coro=<WebRTCConnectionMixin.handle_offer.<locals>._() running at /content/.venv/lib/python3.13/site-packages/fastrtc/webrtc_connection_mixin.py:321> wait_for=<Future pending cb=[Task.__wakeup()]> cb=[AsyncIOEventEmitter._emit_run.<locals>.callback() at /content/.venv/lib/python3.13/site-packages/pyee/asyncio.py:97]>\n"
          ]
        }
      ],
      "source": [
        "!VIRTUAL_ENV=.venv NGROK_AUTH_TOKEN={ngrok_auth_token} uv run --with uvicorn uvicorn app:app"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b308baca-fe39-486c-84b7-1efe914be73a",
      "metadata": {
        "id": "b308baca-fe39-486c-84b7-1efe914be73a"
      },
      "source": [
        "## Echo Server with STT and TTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cd7f487-df11-43b4-9c37-7ad236851600",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cd7f487-df11-43b4-9c37-7ad236851600",
        "outputId": "932ad363-34fa-4e1f-85a1-92222537c114"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "from fastrtc import Stream, ReplyOnPause, get_stt_model, get_tts_model\n",
        "import numpy as np\n",
        "\n",
        "stt_model = get_stt_model() # Moonshine\n",
        "tts_model = get_tts_model() # Kokoro\n",
        "\n",
        "def echo(audio: tuple[int, np.ndarray]):\n",
        "    text = stt_model.stt(audio)\n",
        "    for audio_chunk in tts_model.stream_tts_sync(text):\n",
        "        yield audio_chunk\n",
        "\n",
        "stream = Stream(\n",
        "    handler=ReplyOnPause(echo),\n",
        "    modality=\"audio\",\n",
        "    mode=\"send-receive\",\n",
        "    rtc_configuration={\n",
        "        \"iceServers\": [{ \"urls\": [\"stun:stun.l.google.com:19302\"] }]\n",
        "    }\n",
        ")\n",
        "stream.ui.launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f82ef0f6-a1b3-484a-9e0b-5098bf95a529",
      "metadata": {
        "id": "f82ef0f6-a1b3-484a-9e0b-5098bf95a529"
      },
      "source": [
        "## Running Gradio App"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8a103f1-ce17-4155-b7af-125ef584daf3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8a103f1-ce17-4155-b7af-125ef584daf3",
        "outputId": "711e7b47-e797-45a9-ef74-e019b8b925ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoder_model.onnx: 100% 80.8M/80.8M [00:00<00:00, 170MB/s]\n",
            "decoder_model_merged.onnx: 100% 166M/166M [00:00<00:00, 167MB/s]\n",
            "\u001b[32mINFO\u001b[0m:\t  Warming up STT model.\n",
            "\u001b[32mINFO\u001b[0m:\t  STT model warmed up.\n",
            "kokoro-v1.0.onnx: 100% 326M/326M [00:02<00:00, 143MB/s]\n",
            "voices-v1.0.bin: 100% 28.2M/28.2M [00:00<00:00, 50.7MB/s]\n",
            "\u001b[32mINFO\u001b[0m:\t  Warming up VAD model.\n",
            "\u001b[32mINFO\u001b[0m:\t  VAD model warmed up.\n",
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "* Running on public URL: https://c2436baaab1e532aa9.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://c2436baaab1e532aa9.gradio.live\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!VIRTUAL_ENV=.venv GRADIO_SHARE=\"True\" uv run app.py"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b172de2-29bd-40ea-9a1a-fb143f721f5a",
      "metadata": {
        "id": "0b172de2-29bd-40ea-9a1a-fb143f721f5a"
      },
      "source": [
        "## LangChain App"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7bc6578-0543-4d9a-a620-7f7829835435",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7bc6578-0543-4d9a-a620-7f7829835435",
        "outputId": "4ca9ef1a-55f3-4ad9-a9c6-ae1da66cacfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Resolved 136 packages in 1ms\n",
            "Audited 134 packages in 0.11ms\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "export VIRTUAL_ENV=.venv\n",
        "uv add langchain langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62fb6cee-84d7-4fb8-9fd6-39d37ad4ea17",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62fb6cee-84d7-4fb8-9fd6-39d37ad4ea17",
        "outputId": "4234a1c7-de4b-4eea-b7d7-37022952a6ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "from fastrtc import Stream, ReplyOnPause, get_stt_model, get_tts_model\n",
        "import numpy as np\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "stt_model = get_stt_model() # Moonshine\n",
        "tts_model = get_tts_model() # Kokoro\n",
        "\n",
        "model = init_chat_model(\"openai:gpt-4.1-nano-2025-04-14\")\n",
        "\n",
        "def talk(audio: tuple[int, np.ndarray]):\n",
        "    prompt = stt_model.stt(audio)\n",
        "    response = model.invoke(prompt)\n",
        "    for audio_chunk in tts_model.stream_tts_sync(response.content):\n",
        "        yield audio_chunk\n",
        "\n",
        "stream = Stream(\n",
        "    handler=ReplyOnPause(talk),\n",
        "    modality=\"audio\",\n",
        "    mode=\"send-receive\",\n",
        "    rtc_configuration={\n",
        "        \"iceServers\": [{ \"urls\": [\"stun:stun.l.google.com:19302\"] }]\n",
        "    }\n",
        ")\n",
        "stream.ui.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f1dbf06-cf8f-4a3e-be8f-25404fe23950",
      "metadata": {
        "id": "3f1dbf06-cf8f-4a3e-be8f-25404fe23950"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "api_key = userdata.get('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c296e4d-b2ae-4433-9303-8b61fdb1e1ae",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5c296e4d-b2ae-4433-9303-8b61fdb1e1ae",
        "outputId": "611c745e-8f03-4f2d-edf5-20086f0ccb63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32mINFO\u001b[0m:\t  Warming up STT model.\n",
            "\u001b[32mINFO\u001b[0m:\t  STT model warmed up.\n",
            "\u001b[32mINFO\u001b[0m:\t  Warming up VAD model.\n",
            "\u001b[32mINFO\u001b[0m:\t  VAD model warmed up.\n",
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "* Running on public URL: https://6e81753dafb270a6d9.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n",
            "/root/.local/share/uv/python/cpython-3.13.2-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py:750: RuntimeWarning: coroutine method 'aclose' of 'Kokoro.create_stream' was never awaited\n",
            "  self._ready.clear()\n",
            "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Traceback (most recent call last):\n",
            "  File \u001b[35m\"/content/.venv/lib/python3.13/site-packages/gradio/blocks.py\"\u001b[0m, line \u001b[35m3019\u001b[0m, in \u001b[35mblock_thread\u001b[0m\n",
            "    \u001b[31mtime.sleep\u001b[0m\u001b[1;31m(0.1)\u001b[0m\n",
            "    \u001b[31m~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^\u001b[0m\n",
            "\u001b[1;35mKeyboardInterrupt\u001b[0m\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \u001b[35m\"/content/app.py\"\u001b[0m, line \u001b[35m24\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
            "    \u001b[31mstream.ui.launch\u001b[0m\u001b[1;31m()\u001b[0m\n",
            "    \u001b[31m~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
            "  File \u001b[35m\"/content/.venv/lib/python3.13/site-packages/fastrtc/stream.py\"\u001b[0m, line \u001b[35m261\u001b[0m, in \u001b[35mwrapper\u001b[0m\n",
            "    return callable(*args, **kwargs)\n",
            "  File \u001b[35m\"/content/.venv/lib/python3.13/site-packages/gradio/blocks.py\"\u001b[0m, line \u001b[35m2925\u001b[0m, in \u001b[35mlaunch\u001b[0m\n",
            "    \u001b[31mself.block_thread\u001b[0m\u001b[1;31m()\u001b[0m\n",
            "    \u001b[31m~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
            "  File \u001b[35m\"/content/.venv/lib/python3.13/site-packages/gradio/blocks.py\"\u001b[0m, line \u001b[35m3023\u001b[0m, in \u001b[35mblock_thread\u001b[0m\n",
            "    \u001b[31mself.server.close\u001b[0m\u001b[1;31m()\u001b[0m\n",
            "    \u001b[31m~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
            "  File \u001b[35m\"/content/.venv/lib/python3.13/site-packages/gradio/http_server.py\"\u001b[0m, line \u001b[35m69\u001b[0m, in \u001b[35mclose\u001b[0m\n",
            "    \u001b[31mself.thread.join\u001b[0m\u001b[1;31m(timeout=5)\u001b[0m\n",
            "    \u001b[31m~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^\u001b[0m\n",
            "  File \u001b[35m\"/root/.local/share/uv/python/cpython-3.13.2-linux-x86_64-gnu/lib/python3.13/threading.py\"\u001b[0m, line \u001b[35m1092\u001b[0m, in \u001b[35mjoin\u001b[0m\n",
            "    \u001b[31mself._handle.join\u001b[0m\u001b[1;31m(timeout)\u001b[0m\n",
            "    \u001b[31m~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^\u001b[0m\n",
            "\u001b[1;35mKeyboardInterrupt\u001b[0m\n",
            "Killing tunnel 127.0.0.1:7860 <> https://6e81753dafb270a6d9.gradio.live\n",
            "\u001b[0m<sys>:0: RuntimeWarning: coroutine method 'aclose' of 'Kokoro.create_stream' was never awaited\n",
            "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
            "/root/.local/share/uv/python/cpython-3.13.2-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py:750: RuntimeWarning: coroutine method 'aclose' of 'Kokoro.create_stream' was never awaited\n",
            "  self._ready.clear()\n",
            "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
          ]
        }
      ],
      "source": [
        "!VIRTUAL_ENV=.venv OPENAI_API_KEY={api_key} GRADIO_SHARE=\"True\" uv run app.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fbe346e-a937-42b9-a3e1-7f324bee11aa",
      "metadata": {
        "id": "7fbe346e-a937-42b9-a3e1-7f324bee11aa"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}